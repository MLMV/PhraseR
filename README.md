# PhraseR
Text prediction app for Coursera/Johns Hopkins Data Science Capstone

This repo has all the files used for the Coursera Johns Hopkins University Data Science Capstone project which has the objective to read in a set of large unstructured text files, train a machine learning model and produce a text prediction app on Shinyapps.io.

Links:
- Data download: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
- Exploratory analysis: https://rpubs.com/mlmv/swiftkey
- Presentation: http://rpubs.com/mlmv/phraser
- Shiny app: https://mlmv.shinyapps.io/phraser/

Files in this repo are organized as follows:
- app folder: all files used to produce the Shiny app, including ui.R, server.R, scripts containing word prediction functions with and without smoothing. This folder can be used standalone and should run the app on any PC (at least on Windows 10 with R version 3.4)
  - files folder: .rds files used to read in n-gram MLE and GT probabilities
  - scripts: word prediction scripts for n-gram MLE and GT probabilities
  - www: for the image used in the app
- presentation folder: all the files used for the Rpubs presentation
- project folder: all files used for data mining, exploratory analysis, statistical and predictive modeling.
  - data folder: For the scripts to function, place en_US.blogs.txt, en_US.news.txt and en_US.twitter.txt in this folder.
  - files folder: output folder for the .rds files (used by the app) generated by the models
  - scripts folder: all the R scripts used (see below)
- report folder: all files used for the mid-course progress report and exploratory analysis.

Language processing and predictive modeling:
- Reading, storing, sampling, and cleaning of the data were done with the [tm package](https://cran.r-project.org/web/packages/tm/index.html). 
- [n-gram](https://en.wikipedia.org/wiki/N-gram) frequency tables up to n=6 and [maximum likelihood estimates](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) were created using [package text2vec](https://cran.r-project.org/web/packages/text2vec/index.html).
- [Good-Turing smoothing](https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation) was applied via a custom built function inspired by the work of [Andreas Rubin-Schwarz](https://github.com/andirs) here at Github.
- Two word prediction functions, with and without smoothing, using a [Katz-backoff model](https://en.wikipedia.org/wiki/Katz%27s_back-off_model) were custom built.
- Various timer and profiling functions were used to monitor and optimize runtimes of each component.

Scripts: (folder ./project/scripts)
- Wrapper:
  - R_RUNME.R runs the entire set of scripts and process, from collecting file info, loading and processing the data to producing the word predictions, and takes time stamps for each part.
- Data & text mining:
  - R_fileInfo.R collects file info (file size, nr of words etc) from the 3 data files, renders a csv file stored in the files folder, with statistics that are used later by R_textMining.R to determine sample sizes.
  - R_textMining.R loads the data into a large vcorpus using tm and samples the data down to 10%
  - R_textClean.R tidies up the text including
    - UTF-8to ASCII conversion (needed to be able to use tolower)
    - Removal of punctuation and numbers
    - Conversion to lower caps (only works after ASCII conversion, for some reason)
    - Remove profanities (link inside)
    - Strip excess white spaces
    - Nb. since the purpose of the app is to predict words based on people typing full sentences, stop word removal and stemming were not included
- Statistical analysis / machine learning:
  - R_tokenizer.R builds nGram vocabularies up to n=6 using text2vec, sorts them using dplyr
  - R_goodTuring.R performs Good-Turing smoothing and returns probability tables
  - Nb. R_tokenizer.R and R_goodTuring.R also store their outputs in .rds format in the files folder, for use by the app. The word prediction scripts can use them too, if you load them into memory. (That way you don't have to perform the full text mining each time you want to tweak and test the word prediction scripts.)
  - R_wordPredictionMLE.R uses the MLE tables generated by R_tokenizer.R and creates prediction functions for each n-gram table up to n=6, a Katz-backoff function, and a wrapper function that accepts a user input
  - R_wordPredictionGT.R does the same as R_wordPredictionMLE.R but uses the GT-probability tables generated by R_goodTuring.R
- Optimization / testing
  - R_timers.R lets you look at the timings for each part
  - R_tester.R contains the 20 test sentences from the Coursera quizzes in weeks 3 and 4 in the wrapper function
